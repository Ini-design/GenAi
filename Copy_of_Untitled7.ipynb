{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ini-design/GenAi/blob/main/Copy_of_Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rDV2krm-CHTp"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2uOmx9JwCE07"
      },
      "outputs": [],
      "source": [
        "# Cell 2 - Imports and basic setup\n",
        "import os\n",
        "import getpass\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "\n",
        "# Groq client (uses the groq python package)\n",
        "from groq import Client\n",
        "\n",
        "# Use the API key set in the previous cell\n",
        "client = Client(api_key=GROQ_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gUSA2BLuA50O"
      },
      "outputs": [],
      "source": [
        "# Cell 3 - Helper: use Groq LLM to expand / refine user prompt into a detailed image prompt\n",
        "def expand_prompt_with_groq(user_prompt: str, model: str = \"llama-3.1\") -> str:\n",
        "    \"\"\"\n",
        "    Sends a short user prompt to Groq LLM and requests a high-quality detailed prompt\n",
        "    optimized for image-generation (Stable Diffusion / diffusion models).\n",
        "    \"\"\"\n",
        "    # Minimal system/user style messages -- adjust to your Groq model choice\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant that rewrites short user image requests into detailed, photorealistic prompts for image-generation models.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Rewrite and expand this into a rich image prompt for a diffusion model:\\n\\n{user_prompt}\\n\\nInclude camera, lighting, mood, color palette, style keywords, and useful negative prompts separated by 'NEGATIVE:'\"}\n",
        "    ]\n",
        "    # SDK usage pattern may vary by groq package version; many examples use `chat.completions.create`.\n",
        "    # This call returns a completion; the exact attribute names may differ slightly across versions.\n",
        "    resp = client.chat.completions.create(model=model, messages=messages, max_tokens=512, temperature=0.8)\n",
        "    # Try to access common response structures\n",
        "    text = None\n",
        "    try:\n",
        "        text = resp.choices[0].message[\"content\"]\n",
        "    except Exception:\n",
        "        try:\n",
        "            text = resp.choices[0].text\n",
        "        except Exception:\n",
        "            text = str(resp)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 836,
          "referenced_widgets": [
            "7ad1e444808944e88d5f279f43e0aa89",
            "3049ad4d28cc41708ebea4fb6066b503",
            "4d7f1e5b04444310bfd68689c58e83f6",
            "c7f35e82a03d45a7bab2c87955c70d6d",
            "690f30810acf4c2fa11430b7d0d1feb9",
            "890b46fff6344484a81e699da606b390",
            "fbb7332aa1f14181b4f23b97b72b8e82",
            "6f315b234739411d80d8427f1874e961",
            "45e991d5f5134b5aab4f15d312a8ea67",
            "5dda58fb61384baabd81a10de1958280",
            "c6e94971929749fc8be472dfbe8ba823",
            "2ccd2823bd144656b0d6b19f04869520",
            "a06cf003010e4dc794e15b40256bb08f",
            "c907ea3cdea140ce846ca9e99f5a20e9",
            "3c757f404e9b4a3f824bf08b695953db"
          ]
        },
        "id": "0Eq7HL85DAN_",
        "outputId": "73e8dc18-7906-495f-c8e6-d67f3749a2c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ad1e444808944e88d5f279f43e0aa89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3049ad4d28cc41708ebea4fb6066b503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d7f1e5b04444310bfd68689c58e83f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7f35e82a03d45a7bab2c87955c70d6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "690f30810acf4c2fa11430b7d0d1feb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "890b46fff6344484a81e699da606b390",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbb7332aa1f14181b4f23b97b72b8e82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f315b234739411d80d8427f1874e961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45e991d5f5134b5aab4f15d312a8ea67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dda58fb61384baabd81a10de1958280",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "text_encoder/model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6e94971929749fc8be472dfbe8ba823",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ccd2823bd144656b0d6b19f04869520",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a06cf003010e4dc794e15b40256bb08f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c907ea3cdea140ce846ca9e99f5a20e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c757f404e9b4a3f824bf08b695953db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        }
      ],
      "source": [
        "# Cell 4 - Load Stable Diffusion (adjust model id if you prefer another)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Use float16 when on CUDA for memory savings\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=dtype,\n",
        "    safety_checker=None,   # optionally integrate a safety checker\n",
        "    revision=\"fp16\" if device == \"cuda\" else None\n",
        ")\n",
        "pipe = pipe.to(device)\n",
        "pipe.enable_attention_slicing()  # reduce memory if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fijJmORMDB8i"
      },
      "outputs": [],
      "source": [
        "# Cell 5 - Image generation function (prompt can include \"NEGATIVE:\" block)\n",
        "def generate_image_from_prompt(prompt: str, num_inference_steps: int = 28, guidance_scale: float = 7.5, seed: int | None = None):\n",
        "    # If user provided negative prompts in the same text, split them out\n",
        "    neg = None\n",
        "    if \"NEGATIVE:\" in prompt.upper():\n",
        "        parts = prompt.split(\"NEGATIVE:\")\n",
        "        prompt_pos = parts[0].strip()\n",
        "        neg = parts[1].strip()\n",
        "    else:\n",
        "        prompt_pos = prompt\n",
        "\n",
        "    generator = torch.Generator(device=device)\n",
        "    if seed is not None:\n",
        "        generator = generator.manual_seed(seed)\n",
        "    else:\n",
        "        generator = None\n",
        "\n",
        "    out = pipe(\n",
        "        prompt=prompt_pos,\n",
        "        negative_prompt=neg,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=generator,\n",
        "        height=512,\n",
        "        width=512\n",
        "    )\n",
        "    image = out.images[0]\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHVjl3V6DCZv"
      },
      "outputs": [],
      "source": [
        "# Cell 6 - Gradio UI wiring\n",
        "def run_pipeline(user_text, model_choice=\"llama-3.1\", steps=28, guidance=7.5, seed=None):\n",
        "    expanded = expand_prompt_with_groq(user_text, model=model_choice)\n",
        "    img = generate_image_from_prompt(expanded, num_inference_steps=int(steps), guidance_scale=float(guidance), seed=(int(seed) if seed not in (None, \"\") else None))\n",
        "    buf = BytesIO()\n",
        "    img.save(buf, format=\"PNG\")\n",
        "    buf.seek(0)\n",
        "    return expanded, buf\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Text → Groq LLM prompt expansion → Stable Diffusion image generator\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            prompt_in = gr.Textbox(label=\"User prompt\", placeholder=\"a cozy cottage in a snowy forest, at dawn\")\n",
        "            model_sel = gr.Dropdown(choices=[\"llama-3.1\",\"llama-3.3\",\"mixtral-8x7b\"], value=\"llama-3.1\", label=\"Groq LLM model\")\n",
        "            steps = gr.Slider(10, 50, value=28, step=1, label=\"Inference steps\")\n",
        "            guidance = gr.Slider(1.0, 20.0, value=7.5, step=0.1, label=\"Guidance scale\")\n",
        "            seed_in = gr.Number(value=None, label=\"Seed (optional)\")\n",
        "            gen_btn = gr.Button(\"Generate\")\n",
        "        with gr.Column(scale=3):\n",
        "            expanded_out = gr.Textbox(label=\"Expanded prompt from Groq LLM\")\n",
        "            image_out = gr.Image(label=\"Generated image\")\n",
        "\n",
        "    gen_btn.click(fn=run_pipeline, inputs=[prompt_in, model_sel, steps, guidance, seed_in], outputs=[expanded_out, image_out])\n",
        "\n",
        "demo.launch(share=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoQOln7oonPBAdnCpk/rAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}